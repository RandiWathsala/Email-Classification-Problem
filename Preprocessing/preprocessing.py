# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BXGlY-tb-n4XZGMTsZUYwptQrrv5hbly
"""

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords


# 1. Load Dataset

df = pd.read_csv("fulldataset.csv", encoding='latin-1')

# Inspect first rows and label distribution
print(df.head())
print(df['Labels'].value_counts())


# 2. Clean Labels

# Strip spaces
df['Labels'] = df['Labels'].str.strip()

# Replace spaces with underscores for consistency
df['Labels'] = df['Labels'].replace({
    'Bills and Finance': 'Bills_and_Finance',
    'News and Updates': 'News_and_Updates'
})


# 3. Clean Text (Subject + Body)

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def clean_text(text):
    """
    Clean email text:
    - Lowercase
    - Remove URLs, emails, non-alphabetic characters
    - Remove extra spaces
    - Remove stopwords
    """
    if not isinstance(text, str):
        return ""

    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", " ", text)   # remove urls
    text = re.sub(r"\S+@\S+", " ", text)                   # remove emails
    text = re.sub(r"[^a-zA-Z\s]", " ", text)               # keep only letters
    text = re.sub(r"\s+", " ", text).strip()
    text = " ".join([w for w in text.split() if w not in stop_words])  # remove stopwords
    return text

# Combine subject + body, then clean
df["clean_text"] = (df["subject"].fillna("") + " " + df["body"].fillna("")).apply(clean_text)


# 4. Check for Missing Values

print("Missing values per column:")
print(df.isnull().sum())


# 5. Encode Labels

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df["label_encoded"] = le.fit_transform(df["Labels"])  # FIX: use "Labels" not "label"
print("Classes found:", le.classes_)


# 6. Train-Test Split

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df["clean_text"],
    df["label_encoded"],
    test_size=0.2,
    random_state=42,
    stratify=df["label_encoded"]  # preserves class distribution
)


# 7. TF-IDF Vectorization

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    max_features=5000,   # top 5000 features
    ngram_range=(1, 2)   # unigrams + bigrams
)

X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

print("Train vectorized shape:", X_train_vec.shape)
print("Test vectorized shape:", X_test_vec.shape)

df.to_csv("emails_cleaned.csv", index=False)